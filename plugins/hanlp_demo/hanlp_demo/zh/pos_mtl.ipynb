{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonhwe-yang/Diplomatic-Statement-Scraper/blob/main/plugins/hanlp_demo/hanlp_demo/zh/pos_mtl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfGpInivS0fG"
      },
      "source": [
        "## å®‰è£…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYwV-UkNNzFp"
      },
      "source": [
        "æ— è®ºæ˜¯Windowsã€Linuxè¿˜æ˜¯macOSï¼ŒHanLPçš„å®‰è£…åªéœ€ä¸€å¥è¯æå®šï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1Uf_u7ddMhUt",
        "outputId": "6b67911a-8162-4eb7-897e-c60213cefe8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hanlp\n",
            "  Downloading hanlp-2.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hanlp-common>=0.0.23 (from hanlp)\n",
            "  Downloading hanlp_common-0.0.23.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-downloader (from hanlp)\n",
            "  Downloading hanlp_downloader-0.0.25.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-trie>=0.0.4 (from hanlp)\n",
            "  Downloading hanlp_trie-0.0.5.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (from hanlp) (12.0.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from hanlp) (0.2.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from hanlp) (3.1.0)\n",
            "Collecting toposort==1.5 (from hanlp)\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from hanlp) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from hanlp) (4.55.4)\n",
            "Collecting phrasetree>=0.0.9 (from hanlp-common>=0.0.23->hanlp)\n",
            "  Downloading phrasetree-0.0.9.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml->hanlp) (12.575.51)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.1.1->hanlp) (1.1.8)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->hanlp) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (2025.8.3)\n",
            "Downloading hanlp-2.1.1-py3-none-any.whl (654 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m654.0/654.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: hanlp-common, hanlp-trie, hanlp-downloader, phrasetree\n",
            "  Building wheel for hanlp-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-common: filename=hanlp_common-0.0.23-py3-none-any.whl size=30819 sha256=e2c9a7fdee3a1aadba2391ffe0093254bc0263c9ab339d40d432a7ae0fb98491\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/22/2d/75d505250ca2b83d52998e97db4d3ed89315138752d2e22794\n",
            "  Building wheel for hanlp-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-trie: filename=hanlp_trie-0.0.5-py3-none-any.whl size=6816 sha256=a359c88e763b357fd896c69a18ca86728ba56e8caf42cbebf5083fb81cbf554f\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/a2/e2/b2a458cadfb9ff912848b0e6205bfa5ac628daa5156ccac651\n",
            "  Building wheel for hanlp-downloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-downloader: filename=hanlp_downloader-0.0.25-py3-none-any.whl size=13746 sha256=643255900c41663438f955b3a7ae164d8daaddc9d60c5ead63a245c2f577c150\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/59/2c/1373729f96160c9cded2968f6c33377aab5c95ba1002b559fd\n",
            "  Building wheel for phrasetree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phrasetree: filename=phrasetree-0.0.9-py3-none-any.whl size=44218 sha256=95faac562539cf23a3f9404c73e46d4d8828b73119754d27b0ba372a5e70551d\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/6c/77/51fc144dbf90ff2b9e898ce24082958e63388341de3567ace3\n",
            "Successfully built hanlp-common hanlp-trie hanlp-downloader phrasetree\n",
            "Installing collected packages: toposort, phrasetree, hanlp-common, hanlp-trie, hanlp-downloader, hanlp\n",
            "Successfully installed hanlp-2.1.1 hanlp-common-0.0.23 hanlp-downloader-0.0.25 hanlp-trie-0.0.5 phrasetree-0.0.9 toposort-1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install hanlp -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-1KqEOOJ4t"
      },
      "source": [
        "## åŠ è½½æ¨¡å‹\n",
        "HanLPçš„å·¥ä½œæµç¨‹æ˜¯å…ˆåŠ è½½æ¨¡å‹ï¼Œæ¨¡å‹çš„æ ‡ç¤ºç¬¦å­˜å‚¨åœ¨`hanlp.pretrained`è¿™ä¸ªåŒ…ä¸­ï¼ŒæŒ‰ç…§NLPä»»åŠ¡å½’ç±»ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M7ka0K5OMWU",
        "outputId": "7fb53fba-ff00-4b33-b56e-39742e890242"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small_20201223_035557.zip',\n",
              " 'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base_20201223_201906.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20220626_175100.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_ernie_gram_base_aug_20210904_145403.zip',\n",
              " 'KYOTO_EVAHAN_TOK_LEM_POS_UDEP_LZH': 'https://file.hankcs.com/hanlp/mtl/kyoto_evahan_tok_lem_pos_udep_bert-ancient-chinese_lr_1_aug_dict_20250112_154422.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L6_no_space_20220731_161526.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12_no_space_20220807_133143.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_xlm_base_20220608_003435.zip',\n",
              " 'NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA': 'https://file.hankcs.com/hanlp/mtl/npcmj_ud_kyoto_tok_pos_ner_dep_con_srl_bert_base_char_ja_20210914_133742.zip',\n",
              " 'EN_TOK_LEM_POS_NER_SRL_UDEP_SDP_CON_MODERNBERT_BASE': 'https://file.hankcs.com/hanlp/mtl/en_tok_lem_pos_ner_srl_udep_sdp_con_modernbert_base_prepend_false_20241229_053838.zip',\n",
              " 'EN_TOK_LEM_POS_NER_SRL_UDEP_SDP_CON_MODERNBERT_LARGE': 'https://file.hankcs.com/hanlp/mtl/en_tok_lem_pos_ner_srl_udep_sdp_con_modernbert_large_prepend_false_20250107_181612.zip'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import hanlp\n",
        "hanlp.pretrained.mtl.ALL # MTLå¤šä»»åŠ¡ï¼Œå…·ä½“ä»»åŠ¡è§æ¨¡å‹åç§°ï¼Œè¯­ç§è§åç§°æœ€åä¸€ä¸ªå­—æ®µæˆ–ç›¸åº”è¯­æ–™åº“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMW528wGNulM"
      },
      "source": [
        "è°ƒç”¨`hanlp.load`è¿›è¡ŒåŠ è½½ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ã€‚è‡ªç„¶è¯­è¨€å¤„ç†åˆ†ä¸ºè®¸å¤šä»»åŠ¡ï¼Œåˆ†è¯åªæ˜¯æœ€åˆçº§çš„ä¸€ä¸ªã€‚ä¸å…¶æ¯ä¸ªä»»åŠ¡å•ç‹¬åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œä¸å¦‚åˆ©ç”¨HanLPçš„è”åˆæ¨¡å‹ä¸€æ¬¡æ€§å®Œæˆå¤šä¸ªä»»åŠ¡ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cLFNB-F2ljxh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0tmKBu7sNAXX",
        "outputId": "6f48cf59-5b61-4303-85e5-78fe5edf255e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip\n",
            "100% 467.9 MiB   1.8 MiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl\n",
            "Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip\n",
            "100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers\n",
            "Downloading https://file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip\n",
            "100%  19.4 KiB  19.4 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus\n"
          ]
        }
      ],
      "source": [
        "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elA_UyssOut_"
      },
      "source": [
        "## è¯æ€§æ ‡æ³¨\n",
        "ä»»åŠ¡è¶Šå°‘ï¼Œé€Ÿåº¦è¶Šå¿«ã€‚å¦‚æŒ‡å®šä»…æ‰§è¡Œè¯æ€§æ ‡æ³¨ï¼Œé»˜è®¤CTBæ ‡å‡†ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "BqEmDMGGOtk3",
        "outputId": "e674c7fa-cc02-426d-f68c-c2cb343ecfc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hanlp_common.document.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>hanlp_common.document.Document</b><br/>def __init__(*args, **kwargs) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/hanlp_common/document.py</a>dict() -&gt; new empty dictionary\n",
              "dict(mapping) -&gt; new dictionary initialized from a mapping object&#x27;s\n",
              "    (key, value) pairs\n",
              "dict(iterable) -&gt; new dictionary initialized as if via:\n",
              "    d = {}\n",
              "    for k, v in iterable:\n",
              "        d[k] = v\n",
              "dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs\n",
              "    in the keyword argument list.  For example:  dict(one=1, two=2)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 17);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "a=HanLP(['å¥¹ä»¬æ¯ä¸€ä¸ªäººéƒ½ä½¿æˆ‘<U>å–œæ‚¦</U>ã€æ¬¢ç¬‘ã€å¹¸ç¦ã€çœ‰é£è‰²èˆã€‚', 'æˆ‘çš„å¸Œæœ›æ˜¯å¸Œæœ›å¼ æ™šéœçš„èƒŒå½±è¢«æ™šéœæ˜ çº¢ã€‚'], tasks=['pos'])\n",
        "type(a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    ä»æ–‡æœ¬ä¸­æå–è¢« <U>...</U> æ ‡ç­¾åŒ…è£¹çš„ç›®æ ‡è¯ï¼Œå¹¶ç§»é™¤æ‰€æœ‰ <B>...</B> æ ‡ç­¾åŠå…¶å†…å®¹ã€‚\n",
        "    ä½¿ç”¨å ä½ç¬¦æ³•æ¥ç²¾ç¡®è®¡ç®—ç›®æ ‡è¯åœ¨æœ€ç»ˆæ–‡æœ¬ä¸­çš„ä½ç½®ã€‚\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\", (0, 0), \"\"\n",
        "\n",
        "    # 1. ç§»é™¤ <B>...</B> åŠå…¶å†…éƒ¨çš„æ‰€æœ‰å†…å®¹\n",
        "    text_no_b = re.sub(r\"<B>.*?</B>\", \"\", text)\n",
        "\n",
        "    # 2. æŸ¥æ‰¾ <U>...</U> å¹¶ç”¨ä¸€ä¸ªå”¯ä¸€çš„å ä½ç¬¦æ›¿æ¢ï¼ŒåŒæ—¶è®°å½•ç›®æ ‡è¯\n",
        "    placeholder = \"__TARGET_WORD__\"\n",
        "    u_match = re.search(r\"<U>(.*?)</U>\", text_no_b)\n",
        "\n",
        "    if u_match:\n",
        "        target = u_match.group(1)  # æå–ç›®æ ‡è¯\n",
        "        # å°† <U>...</U> æ›¿æ¢ä¸ºå ä½ç¬¦\n",
        "        temp_text = re.sub(r\"<U>.*?</U>\", placeholder, text_no_b)\n",
        "\n",
        "        # 3. è®¡ç®—å ä½ç¬¦åœ¨ temp_text ä¸­çš„ä½ç½®\n",
        "        start = temp_text.find(placeholder)\n",
        "        if start != -1:\n",
        "            span = (start, start + len(target))\n",
        "            # 4. å°†å ä½ç¬¦æ›¿æ¢å›ç›®æ ‡è¯ï¼Œå¾—åˆ°æœ€ç»ˆçº¯æ–‡æœ¬\n",
        "            plain_text = temp_text.replace(placeholder, target)\n",
        "            return target, span, plain_text\n",
        "        else:\n",
        "            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿ\n",
        "            return \"\", (0, 0), text_no_b\n",
        "    else:\n",
        "        # æ²¡æ‰¾åˆ° <U>...</U>\n",
        "        return \"\", (0, 0), text_no_b"
      ],
      "metadata": {
        "id": "AdTqG5BFrLg_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_target_noun(text):\n",
        "    \"\"\"\n",
        "    åˆ¤æ–­æ–‡æœ¬ä¸­è¢«<U>...</U>æ ‡è®°çš„ç›®æ ‡è¯æ˜¯å¦ä¸ºåè¯ã€‚\n",
        "\n",
        "    Args:\n",
        "        text (str): åŒ…å«æ ‡ç­¾çš„åŸå§‹æ–‡æœ¬ã€‚\n",
        "\n",
        "    Returns:\n",
        "        bool: å¦‚æœç›®æ ‡è¯æ˜¯åè¯åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚\n",
        "    \"\"\"\n",
        "    target, span, plain_text = preprocess(text)\n",
        "    if not target or not plain_text.strip():\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # è°ƒç”¨ HanLP è¿›è¡Œè¯æ€§æ ‡æ³¨\n",
        "        doc = HanLP(plain_text, tasks='pos')\n",
        "\n",
        "        # ä½¿ç”¨ 'tok/fine' å’Œ 'pos/ctb'\n",
        "        tokens = doc['tok/fine']\n",
        "        pos_tags = doc['pos/ctb']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è°ƒç”¨ HanLP å¤±è´¥: {e}\")\n",
        "        return False\n",
        "\n",
        "    # å®šä½ç›®æ ‡è¯å¹¶åˆ¤æ–­æ˜¯å¦ä¸ºåè¯\n",
        "    current_position = 0\n",
        "    target_found = False\n",
        "\n",
        "    for token, pos in zip(tokens, pos_tags):\n",
        "        token_length = len(token)\n",
        "        token_start = current_position\n",
        "        token_end = current_position + token_length\n",
        "\n",
        "        # æ£€æŸ¥ç›®æ ‡è¯çš„èµ·å§‹ä½ç½®æ˜¯å¦è½åœ¨å½“å‰ token çš„èŒƒå›´å†…\n",
        "        if token_start <= span[0] < token_end:\n",
        "            target_found = True\n",
        "            # å¦‚æœè¯æ€§ä»¥ 'N' å¼€å¤´ï¼Œåˆ™è®¤ä¸ºæ˜¯åè¯\n",
        "            if pos.startswith(\"N\"):\n",
        "                return True\n",
        "            break\n",
        "\n",
        "        current_position = token_end\n",
        "\n",
        "    # å¦‚æœæœªæ‰¾åˆ°ç›®æ ‡è¯æˆ–å…¶è¯æ€§ä¸æ˜¯åè¯\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "q5C_erNWrh9Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_excel_nouns(input_file, output_file, column_name=\"å†…å®¹\"):\n",
        "    \"\"\"\n",
        "    è¯»å–Excelæ–‡ä»¶ï¼Œç­›é€‰å‡ºç›®æ ‡è¯ä¸ºåè¯çš„è¡Œï¼Œå¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶ã€‚\n",
        "\n",
        "    Args:\n",
        "        input_file (str): è¾“å…¥çš„Excelæ–‡ä»¶è·¯å¾„ã€‚\n",
        "        output_file (str): è¾“å‡ºçš„Excelæ–‡ä»¶è·¯å¾„ã€‚\n",
        "        column_name (str): åŒ…å«æ–‡æœ¬å†…å®¹çš„åˆ—åï¼Œé»˜è®¤ä¸º\"å†…å®¹\"ã€‚\n",
        "    \"\"\"\n",
        "    # 1. è¯»å–Excelæ–‡ä»¶\n",
        "    try:\n",
        "        df = pd.read_excel(input_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
        "        return\n",
        "\n",
        "    if column_name not in df.columns:\n",
        "        print(f\"âŒ åˆ—å '{column_name}' ä¸å­˜åœ¨äºæ–‡ä»¶ä¸­ã€‚å¯ç”¨çš„åˆ—æœ‰: {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # 2. åˆ›å»ºä¸€ä¸ªæ–°åˆ—æ¥å­˜å‚¨åˆ¤æ–­ç»“æœ\n",
        "    print(\"æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™...\")\n",
        "    df['is_noun'] = df[column_name].apply(is_target_noun)\n",
        "\n",
        "    # 3. ç­›é€‰å‡ºç›®æ ‡è¯æ˜¯åè¯çš„è¡Œ\n",
        "    noun_df = df[df['is_noun']].copy()\n",
        "\n",
        "    # 4. åˆ é™¤ä¸´æ—¶çš„ is_noun åˆ—\n",
        "    noun_df.drop(columns=['is_noun'], inplace=True)\n",
        "\n",
        "    # 5. ä¿å­˜åˆ°æ–°çš„Excelæ–‡ä»¶\n",
        "    try:\n",
        "        noun_df.to_excel(output_file, index=False)\n",
        "        print(f\"âœ… å¤„ç†å®Œæˆï¼å·²å°†ç›®æ ‡è¯ä¸ºåè¯çš„è¡Œä¿å­˜åˆ°: {output_file}\")\n",
        "        print(f\"ğŸ“Š åŸå§‹æ•°æ®å…± {len(df)} è¡Œï¼Œç­›é€‰åå‰©ä½™ {len(noun_df)} è¡Œã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}\")"
      ],
      "metadata": {
        "id": "2IE0iUHHFokM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ä½¿ç”¨ç¤ºä¾‹ ===\n",
        "if __name__ == \"__main__\":\n",
        "    # è®¾ç½®ä½ çš„æ–‡ä»¶è·¯å¾„\n",
        "    input_excel = \"/content/æ‚²ä¼¤ æ–‡å­¦.xlsx\"  # ä¿®æ”¹ä¸ºä½ çš„è¾“å…¥æ–‡ä»¶è·¯å¾„\n",
        "    output_excel = \"/content/æ‚²ä¼¤æ–‡å­¦_åè¯ç­›é€‰ç»“æœ.xlsx\"  # ä¿®æ”¹ä¸ºä½ çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
        "    column = \"å†…å®¹\"  # ä½ çš„Excelä¸­å­˜æ”¾æ–‡æœ¬çš„åˆ—å\n",
        "\n",
        "    # æ‰§è¡Œç­›é€‰\n",
        "    filter_excel_nouns(input_excel, output_excel, column)"
      ],
      "metadata": {
        "id": "XB6kPAjOrMtZ",
        "outputId": "d0cdfd48-b1a3-4c2d-f459-128b9ad0efdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™...\n",
            "âœ… å¤„ç†å®Œæˆï¼å·²å°†ç›®æ ‡è¯ä¸ºåè¯çš„è¡Œä¿å­˜åˆ°: /content/æ‚²ä¼¤æ–‡å­¦_åè¯ç­›é€‰ç»“æœ.xlsx\n",
            "ğŸ“Š åŸå§‹æ•°æ®å…± 3858 è¡Œï¼Œç­›é€‰åå‰©ä½™ 1621 è¡Œã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    ä»æ–‡æœ¬ä¸­æå–è¢« <U>...</U> æ ‡ç­¾åŒ…è£¹çš„ç›®æ ‡è¯ï¼Œå¹¶ç§»é™¤æ‰€æœ‰ <B>...</B> æ ‡ç­¾åŠå…¶å†…å®¹ã€‚\n",
        "    ä½¿ç”¨å ä½ç¬¦æ³•æ¥ç²¾ç¡®è®¡ç®—ç›®æ ‡è¯åœ¨æœ€ç»ˆæ–‡æœ¬ä¸­çš„ä½ç½®ã€‚\n",
        "    \"\"\"\n",
        "    # 1. ç§»é™¤ <B>...</B> åŠå…¶å†…éƒ¨çš„æ‰€æœ‰å†…å®¹\n",
        "    text_no_b = re.sub(r\"<B>.*?</B>\", \"\", text)\n",
        "\n",
        "    # 2. æŸ¥æ‰¾ <U>...</U> å¹¶ç”¨ä¸€ä¸ªå”¯ä¸€çš„å ä½ç¬¦æ›¿æ¢ï¼ŒåŒæ—¶è®°å½•ç›®æ ‡è¯\n",
        "    placeholder = \"__TARGET_WORD__\"\n",
        "    u_match = re.search(r\"<U>(.*?)</U>\", text_no_b)\n",
        "\n",
        "    if u_match:\n",
        "        target = u_match.group(1)  # æå–ç›®æ ‡è¯\n",
        "        # å°† <U>...</U> æ›¿æ¢ä¸ºå ä½ç¬¦\n",
        "        temp_text = re.sub(r\"<U>.*?</U>\", placeholder, text_no_b)\n",
        "\n",
        "        # 3. è®¡ç®—å ä½ç¬¦åœ¨ temp_text ä¸­çš„ä½ç½®\n",
        "        start = temp_text.find(placeholder)\n",
        "        if start != -1:\n",
        "            span = (start, start + len(target))\n",
        "            # 4. å°†å ä½ç¬¦æ›¿æ¢å›ç›®æ ‡è¯ï¼Œå¾—åˆ°æœ€ç»ˆçº¯æ–‡æœ¬\n",
        "            plain_text = temp_text.replace(placeholder, target)\n",
        "            return target, span, plain_text\n",
        "        else:\n",
        "            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿ\n",
        "            return \"\", (0, 0), text_no_b\n",
        "    else:\n",
        "        # æ²¡æ‰¾åˆ° <U>...</U>\n",
        "        return \"\", (0, 0), text_no_b\n",
        "\n",
        "def debug_single_sentence(text):\n",
        "    \"\"\"\n",
        "    è°ƒè¯•å•ä¸ªå¥å­ï¼Œæ£€æŸ¥è¢« <U>...</U> æ ‡ç­¾åŒ…è£¹çš„ç›®æ ‡è¯æ˜¯å¦è¢«è¯†åˆ«ä¸ºåè¯ã€‚\n",
        "    \"\"\"\n",
        "    target, span, plain_text = preprocess(text)\n",
        "    if not target:\n",
        "        print(\"âŒ å¥å­ä¸­æœªæ‰¾åˆ°<U>åŒ…è£¹çš„ç›®æ ‡è¯\")\n",
        "        return None\n",
        "\n",
        "    print(f\"åŸå¥: {text}\")\n",
        "    print(f\"å¤„ç†åæ–‡æœ¬: {plain_text}\")\n",
        "    print(f\"ç›®æ ‡è¯: {target}, åœ¨å¤„ç†åæ–‡æœ¬ä¸­çš„ä½ç½®: {span}\")\n",
        "\n",
        "    try:\n",
        "        # è°ƒç”¨ HanLP è¿›è¡Œè¯æ€§æ ‡æ³¨\n",
        "        doc = HanLP(plain_text, tasks='pos')\n",
        "\n",
        "        # æ ¹æ®ä½ å®æµ‹çš„ç»“æœï¼Œä½¿ç”¨ 'tok/fine' å’Œ 'pos/ctb'\n",
        "        tokens = doc['tok/fine']\n",
        "        pos_tags = doc['pos/ctb']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è°ƒç”¨ HanLP å¤±è´¥: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(\"åˆ†è¯/è¯æ€§:\")\n",
        "    print(\" \".join(f\"{tok}/{pos}\" for tok, pos in zip(tokens, pos_tags)))\n",
        "\n",
        "    # å®šä½ç›®æ ‡è¯å¹¶åˆ¤æ–­æ˜¯å¦ä¸ºåè¯\n",
        "    cursor = 0\n",
        "    is_noun = False\n",
        "    target_found = False\n",
        "\n",
        "    for token, pos in zip(tokens, pos_tags):\n",
        "        token_start = cursor\n",
        "        token_end = cursor + len(token)\n",
        "\n",
        "        if token_start <= span[0] < token_end:\n",
        "            target_found = True\n",
        "            print(f\"ğŸ” æ‰¾åˆ°ç›®æ ‡è¯æ‰€åœ¨åˆ†è¯: '{token}', è¯æ€§: {pos}\")\n",
        "            if pos.startswith(\"N\"):\n",
        "                is_noun = True\n",
        "            break\n",
        "\n",
        "        cursor = token_end\n",
        "\n",
        "    if not target_found:\n",
        "        print(\"âŒ æœªèƒ½åœ¨åˆ†è¯ç»“æœä¸­å®šä½åˆ°ç›®æ ‡è¯\")\n",
        "        return None\n",
        "\n",
        "    print(f\"ç›®æ ‡è¯ '{target}' æ˜¯å¦åè¯: {'âœ… æ˜¯' if is_noun else 'âŒ å¦'}\")\n",
        "    return is_noun\n",
        "\n",
        "# === æµ‹è¯• ===\n",
        "if __name__ == \"__main__\":\n",
        "    test_text = \"<B>äººæ°‘æ—¥æŠ¥1989å¹´04æœˆ26æ—¥</B>ä¸¾å›½<U>æ‚²ä¼¤</U>\"\n",
        "    result = debug_single_sentence(test_text)"
      ],
      "metadata": {
        "id": "j2V0deP9rx-h",
        "outputId": "624b4c35-0bf6-48f2-8cfc-9a36274a455d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "åŸå¥: <B>äººæ°‘æ—¥æŠ¥1989å¹´04æœˆ26æ—¥</B>ä¸¾å›½<U>æ‚²ä¼¤</U>\n",
            "å¤„ç†åæ–‡æœ¬: ä¸¾å›½æ‚²ä¼¤\n",
            "ç›®æ ‡è¯: æ‚²ä¼¤, åœ¨å¤„ç†åæ–‡æœ¬ä¸­çš„ä½ç½®: (2, 4)\n",
            "åˆ†è¯/è¯æ€§:\n",
            "ä¸¾å›½/NN æ‚²ä¼¤/VA\n",
            "ğŸ” æ‰¾åˆ°ç›®æ ‡è¯æ‰€åœ¨åˆ†è¯: 'æ‚²ä¼¤', è¯æ€§: VA\n",
            "ç›®æ ‡è¯ 'æ‚²ä¼¤' æ˜¯å¦åè¯: âŒ å¦\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pos_mtl.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}