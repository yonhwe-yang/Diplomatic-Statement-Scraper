{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonhwe-yang/Diplomatic-Statement-Scraper/blob/main/plugins/hanlp_demo/hanlp_demo/zh/pos_mtl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfGpInivS0fG"
      },
      "source": [
        "## 安装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYwV-UkNNzFp"
      },
      "source": [
        "无论是Windows、Linux还是macOS，HanLP的安装只需一句话搞定："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1Uf_u7ddMhUt",
        "outputId": "6b67911a-8162-4eb7-897e-c60213cefe8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hanlp\n",
            "  Downloading hanlp-2.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hanlp-common>=0.0.23 (from hanlp)\n",
            "  Downloading hanlp_common-0.0.23.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-downloader (from hanlp)\n",
            "  Downloading hanlp_downloader-0.0.25.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-trie>=0.0.4 (from hanlp)\n",
            "  Downloading hanlp_trie-0.0.5.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (from hanlp) (12.0.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from hanlp) (0.2.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from hanlp) (3.1.0)\n",
            "Collecting toposort==1.5 (from hanlp)\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from hanlp) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from hanlp) (4.55.4)\n",
            "Collecting phrasetree>=0.0.9 (from hanlp-common>=0.0.23->hanlp)\n",
            "  Downloading phrasetree-0.0.9.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml->hanlp) (12.575.51)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.1.1->hanlp) (1.1.8)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->hanlp) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (2025.8.3)\n",
            "Downloading hanlp-2.1.1-py3-none-any.whl (654 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m654.0/654.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: hanlp-common, hanlp-trie, hanlp-downloader, phrasetree\n",
            "  Building wheel for hanlp-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-common: filename=hanlp_common-0.0.23-py3-none-any.whl size=30819 sha256=e2c9a7fdee3a1aadba2391ffe0093254bc0263c9ab339d40d432a7ae0fb98491\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/22/2d/75d505250ca2b83d52998e97db4d3ed89315138752d2e22794\n",
            "  Building wheel for hanlp-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-trie: filename=hanlp_trie-0.0.5-py3-none-any.whl size=6816 sha256=a359c88e763b357fd896c69a18ca86728ba56e8caf42cbebf5083fb81cbf554f\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/a2/e2/b2a458cadfb9ff912848b0e6205bfa5ac628daa5156ccac651\n",
            "  Building wheel for hanlp-downloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-downloader: filename=hanlp_downloader-0.0.25-py3-none-any.whl size=13746 sha256=643255900c41663438f955b3a7ae164d8daaddc9d60c5ead63a245c2f577c150\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/59/2c/1373729f96160c9cded2968f6c33377aab5c95ba1002b559fd\n",
            "  Building wheel for phrasetree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phrasetree: filename=phrasetree-0.0.9-py3-none-any.whl size=44218 sha256=95faac562539cf23a3f9404c73e46d4d8828b73119754d27b0ba372a5e70551d\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/6c/77/51fc144dbf90ff2b9e898ce24082958e63388341de3567ace3\n",
            "Successfully built hanlp-common hanlp-trie hanlp-downloader phrasetree\n",
            "Installing collected packages: toposort, phrasetree, hanlp-common, hanlp-trie, hanlp-downloader, hanlp\n",
            "Successfully installed hanlp-2.1.1 hanlp-common-0.0.23 hanlp-downloader-0.0.25 hanlp-trie-0.0.5 phrasetree-0.0.9 toposort-1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install hanlp -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-1KqEOOJ4t"
      },
      "source": [
        "## 加载模型\n",
        "HanLP的工作流程是先加载模型，模型的标示符存储在`hanlp.pretrained`这个包中，按照NLP任务归类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M7ka0K5OMWU",
        "outputId": "7fb53fba-ff00-4b33-b56e-39742e890242"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small_20201223_035557.zip',\n",
              " 'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base_20201223_201906.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20220626_175100.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_ernie_gram_base_aug_20210904_145403.zip',\n",
              " 'KYOTO_EVAHAN_TOK_LEM_POS_UDEP_LZH': 'https://file.hankcs.com/hanlp/mtl/kyoto_evahan_tok_lem_pos_udep_bert-ancient-chinese_lr_1_aug_dict_20250112_154422.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L6_no_space_20220731_161526.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12_no_space_20220807_133143.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_xlm_base_20220608_003435.zip',\n",
              " 'NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA': 'https://file.hankcs.com/hanlp/mtl/npcmj_ud_kyoto_tok_pos_ner_dep_con_srl_bert_base_char_ja_20210914_133742.zip',\n",
              " 'EN_TOK_LEM_POS_NER_SRL_UDEP_SDP_CON_MODERNBERT_BASE': 'https://file.hankcs.com/hanlp/mtl/en_tok_lem_pos_ner_srl_udep_sdp_con_modernbert_base_prepend_false_20241229_053838.zip',\n",
              " 'EN_TOK_LEM_POS_NER_SRL_UDEP_SDP_CON_MODERNBERT_LARGE': 'https://file.hankcs.com/hanlp/mtl/en_tok_lem_pos_ner_srl_udep_sdp_con_modernbert_large_prepend_false_20250107_181612.zip'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import hanlp\n",
        "hanlp.pretrained.mtl.ALL # MTL多任务，具体任务见模型名称，语种见名称最后一个字段或相应语料库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMW528wGNulM"
      },
      "source": [
        "调用`hanlp.load`进行加载，模型会自动下载到本地缓存。自然语言处理分为许多任务，分词只是最初级的一个。与其每个任务单独创建一个模型，不如利用HanLP的联合模型一次性完成多个任务："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cLFNB-F2ljxh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0tmKBu7sNAXX",
        "outputId": "6f48cf59-5b61-4303-85e5-78fe5edf255e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip\n",
            "100% 467.9 MiB   1.8 MiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl\n",
            "Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip\n",
            "100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers\n",
            "Downloading https://file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip\n",
            "100%  19.4 KiB  19.4 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus\n"
          ]
        }
      ],
      "source": [
        "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elA_UyssOut_"
      },
      "source": [
        "## 词性标注\n",
        "任务越少，速度越快。如指定仅执行词性标注，默认CTB标准："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "BqEmDMGGOtk3",
        "outputId": "e674c7fa-cc02-426d-f68c-c2cb343ecfc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hanlp_common.document.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>hanlp_common.document.Document</b><br/>def __init__(*args, **kwargs) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/hanlp_common/document.py</a>dict() -&gt; new empty dictionary\n",
              "dict(mapping) -&gt; new dictionary initialized from a mapping object&#x27;s\n",
              "    (key, value) pairs\n",
              "dict(iterable) -&gt; new dictionary initialized as if via:\n",
              "    d = {}\n",
              "    for k, v in iterable:\n",
              "        d[k] = v\n",
              "dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs\n",
              "    in the keyword argument list.  For example:  dict(one=1, two=2)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 17);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "a=HanLP(['她们每一个人都使我<U>喜悦</U>、欢笑、幸福、眉飞色舞。', '我的希望是希望张晚霞的背影被晚霞映红。'], tasks=['pos'])\n",
        "type(a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    从文本中提取被 <U>...</U> 标签包裹的目标词，并移除所有 <B>...</B> 标签及其内容。\n",
        "    使用占位符法来精确计算目标词在最终文本中的位置。\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\", (0, 0), \"\"\n",
        "\n",
        "    # 1. 移除 <B>...</B> 及其内部的所有内容\n",
        "    text_no_b = re.sub(r\"<B>.*?</B>\", \"\", text)\n",
        "\n",
        "    # 2. 查找 <U>...</U> 并用一个唯一的占位符替换，同时记录目标词\n",
        "    placeholder = \"__TARGET_WORD__\"\n",
        "    u_match = re.search(r\"<U>(.*?)</U>\", text_no_b)\n",
        "\n",
        "    if u_match:\n",
        "        target = u_match.group(1)  # 提取目标词\n",
        "        # 将 <U>...</U> 替换为占位符\n",
        "        temp_text = re.sub(r\"<U>.*?</U>\", placeholder, text_no_b)\n",
        "\n",
        "        # 3. 计算占位符在 temp_text 中的位置\n",
        "        start = temp_text.find(placeholder)\n",
        "        if start != -1:\n",
        "            span = (start, start + len(target))\n",
        "            # 4. 将占位符替换回目标词，得到最终纯文本\n",
        "            plain_text = temp_text.replace(placeholder, target)\n",
        "            return target, span, plain_text\n",
        "        else:\n",
        "            # 理论上不会发生\n",
        "            return \"\", (0, 0), text_no_b\n",
        "    else:\n",
        "        # 没找到 <U>...</U>\n",
        "        return \"\", (0, 0), text_no_b"
      ],
      "metadata": {
        "id": "AdTqG5BFrLg_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_target_noun(text):\n",
        "    \"\"\"\n",
        "    判断文本中被<U>...</U>标记的目标词是否为名词。\n",
        "\n",
        "    Args:\n",
        "        text (str): 包含标签的原始文本。\n",
        "\n",
        "    Returns:\n",
        "        bool: 如果目标词是名词则返回 True，否则返回 False。\n",
        "    \"\"\"\n",
        "    target, span, plain_text = preprocess(text)\n",
        "    if not target or not plain_text.strip():\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # 调用 HanLP 进行词性标注\n",
        "        doc = HanLP(plain_text, tasks='pos')\n",
        "\n",
        "        # 使用 'tok/fine' 和 'pos/ctb'\n",
        "        tokens = doc['tok/fine']\n",
        "        pos_tags = doc['pos/ctb']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 调用 HanLP 失败: {e}\")\n",
        "        return False\n",
        "\n",
        "    # 定位目标词并判断是否为名词\n",
        "    current_position = 0\n",
        "    target_found = False\n",
        "\n",
        "    for token, pos in zip(tokens, pos_tags):\n",
        "        token_length = len(token)\n",
        "        token_start = current_position\n",
        "        token_end = current_position + token_length\n",
        "\n",
        "        # 检查目标词的起始位置是否落在当前 token 的范围内\n",
        "        if token_start <= span[0] < token_end:\n",
        "            target_found = True\n",
        "            # 如果词性以 'N' 开头，则认为是名词\n",
        "            if pos.startswith(\"N\"):\n",
        "                return True\n",
        "            break\n",
        "\n",
        "        current_position = token_end\n",
        "\n",
        "    # 如果未找到目标词或其词性不是名词\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "q5C_erNWrh9Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_excel_nouns(input_file, output_file, column_name=\"内容\"):\n",
        "    \"\"\"\n",
        "    读取Excel文件，筛选出目标词为名词的行，并保存到新文件。\n",
        "\n",
        "    Args:\n",
        "        input_file (str): 输入的Excel文件路径。\n",
        "        output_file (str): 输出的Excel文件路径。\n",
        "        column_name (str): 包含文本内容的列名，默认为\"内容\"。\n",
        "    \"\"\"\n",
        "    # 1. 读取Excel文件\n",
        "    try:\n",
        "        df = pd.read_excel(input_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ 找不到文件: {input_file}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 读取Excel文件时出错: {e}\")\n",
        "        return\n",
        "\n",
        "    if column_name not in df.columns:\n",
        "        print(f\"❌ 列名 '{column_name}' 不存在于文件中。可用的列有: {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # 2. 创建一个新列来存储判断结果\n",
        "    print(\"正在处理数据，请稍候...\")\n",
        "    df['is_noun'] = df[column_name].apply(is_target_noun)\n",
        "\n",
        "    # 3. 筛选出目标词是名词的行\n",
        "    noun_df = df[df['is_noun']].copy()\n",
        "\n",
        "    # 4. 删除临时的 is_noun 列\n",
        "    noun_df.drop(columns=['is_noun'], inplace=True)\n",
        "\n",
        "    # 5. 保存到新的Excel文件\n",
        "    try:\n",
        "        noun_df.to_excel(output_file, index=False)\n",
        "        print(f\"✅ 处理完成！已将目标词为名词的行保存到: {output_file}\")\n",
        "        print(f\"📊 原始数据共 {len(df)} 行，筛选后剩余 {len(noun_df)} 行。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 保存文件时出错: {e}\")"
      ],
      "metadata": {
        "id": "2IE0iUHHFokM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 使用示例 ===\n",
        "if __name__ == \"__main__\":\n",
        "    # 设置你的文件路径\n",
        "    input_excel = \"/content/悲伤 文学.xlsx\"  # 修改为你的输入文件路径\n",
        "    output_excel = \"/content/悲伤文学_名词筛选结果.xlsx\"  # 修改为你的输出文件路径\n",
        "    column = \"内容\"  # 你的Excel中存放文本的列名\n",
        "\n",
        "    # 执行筛选\n",
        "    filter_excel_nouns(input_excel, output_excel, column)"
      ],
      "metadata": {
        "id": "XB6kPAjOrMtZ",
        "outputId": "d0cdfd48-b1a3-4c2d-f459-128b9ad0efdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在处理数据，请稍候...\n",
            "✅ 处理完成！已将目标词为名词的行保存到: /content/悲伤文学_名词筛选结果.xlsx\n",
            "📊 原始数据共 3858 行，筛选后剩余 1621 行。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    从文本中提取被 <U>...</U> 标签包裹的目标词，并移除所有 <B>...</B> 标签及其内容。\n",
        "    使用占位符法来精确计算目标词在最终文本中的位置。\n",
        "    \"\"\"\n",
        "    # 1. 移除 <B>...</B> 及其内部的所有内容\n",
        "    text_no_b = re.sub(r\"<B>.*?</B>\", \"\", text)\n",
        "\n",
        "    # 2. 查找 <U>...</U> 并用一个唯一的占位符替换，同时记录目标词\n",
        "    placeholder = \"__TARGET_WORD__\"\n",
        "    u_match = re.search(r\"<U>(.*?)</U>\", text_no_b)\n",
        "\n",
        "    if u_match:\n",
        "        target = u_match.group(1)  # 提取目标词\n",
        "        # 将 <U>...</U> 替换为占位符\n",
        "        temp_text = re.sub(r\"<U>.*?</U>\", placeholder, text_no_b)\n",
        "\n",
        "        # 3. 计算占位符在 temp_text 中的位置\n",
        "        start = temp_text.find(placeholder)\n",
        "        if start != -1:\n",
        "            span = (start, start + len(target))\n",
        "            # 4. 将占位符替换回目标词，得到最终纯文本\n",
        "            plain_text = temp_text.replace(placeholder, target)\n",
        "            return target, span, plain_text\n",
        "        else:\n",
        "            # 理论上不会发生\n",
        "            return \"\", (0, 0), text_no_b\n",
        "    else:\n",
        "        # 没找到 <U>...</U>\n",
        "        return \"\", (0, 0), text_no_b\n",
        "\n",
        "def debug_single_sentence(text):\n",
        "    \"\"\"\n",
        "    调试单个句子，检查被 <U>...</U> 标签包裹的目标词是否被识别为名词。\n",
        "    \"\"\"\n",
        "    target, span, plain_text = preprocess(text)\n",
        "    if not target:\n",
        "        print(\"❌ 句子中未找到<U>包裹的目标词\")\n",
        "        return None\n",
        "\n",
        "    print(f\"原句: {text}\")\n",
        "    print(f\"处理后文本: {plain_text}\")\n",
        "    print(f\"目标词: {target}, 在处理后文本中的位置: {span}\")\n",
        "\n",
        "    try:\n",
        "        # 调用 HanLP 进行词性标注\n",
        "        doc = HanLP(plain_text, tasks='pos')\n",
        "\n",
        "        # 根据你实测的结果，使用 'tok/fine' 和 'pos/ctb'\n",
        "        tokens = doc['tok/fine']\n",
        "        pos_tags = doc['pos/ctb']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 调用 HanLP 失败: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(\"分词/词性:\")\n",
        "    print(\" \".join(f\"{tok}/{pos}\" for tok, pos in zip(tokens, pos_tags)))\n",
        "\n",
        "    # 定位目标词并判断是否为名词\n",
        "    cursor = 0\n",
        "    is_noun = False\n",
        "    target_found = False\n",
        "\n",
        "    for token, pos in zip(tokens, pos_tags):\n",
        "        token_start = cursor\n",
        "        token_end = cursor + len(token)\n",
        "\n",
        "        if token_start <= span[0] < token_end:\n",
        "            target_found = True\n",
        "            print(f\"🔍 找到目标词所在分词: '{token}', 词性: {pos}\")\n",
        "            if pos.startswith(\"N\"):\n",
        "                is_noun = True\n",
        "            break\n",
        "\n",
        "        cursor = token_end\n",
        "\n",
        "    if not target_found:\n",
        "        print(\"❌ 未能在分词结果中定位到目标词\")\n",
        "        return None\n",
        "\n",
        "    print(f\"目标词 '{target}' 是否名词: {'✅ 是' if is_noun else '❌ 否'}\")\n",
        "    return is_noun\n",
        "\n",
        "# === 测试 ===\n",
        "if __name__ == \"__main__\":\n",
        "    test_text = \"<B>人民日报1989年04月26日</B>举国<U>悲伤</U>\"\n",
        "    result = debug_single_sentence(test_text)"
      ],
      "metadata": {
        "id": "j2V0deP9rx-h",
        "outputId": "624b4c35-0bf6-48f2-8cfc-9a36274a455d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原句: <B>人民日报1989年04月26日</B>举国<U>悲伤</U>\n",
            "处理后文本: 举国悲伤\n",
            "目标词: 悲伤, 在处理后文本中的位置: (2, 4)\n",
            "分词/词性:\n",
            "举国/NN 悲伤/VA\n",
            "🔍 找到目标词所在分词: '悲伤', 词性: VA\n",
            "目标词 '悲伤' 是否名词: ❌ 否\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pos_mtl.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}